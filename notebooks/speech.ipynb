{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OKWIa7PbT9Ak",
    "outputId": "41081d78-a044-4e00-aa16-a724cffbb5f4"
   },
   "outputs": [],
   "source": [
    "# Install jiwer for WER evaluation\n",
    "!pip install jiwer\n",
    "\n",
    "# download & extract into /content/LJSpeech\n",
    "!mkdir -p /content/LJSpeech\n",
    "!wget -P /content/LJSpeech \\\n",
    "    https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
    "!tar xvjf /content/LJSpeech/LJSpeech-1.1.tar.bz2 \\\n",
    "       -C /content/LJSpeech\n",
    "\n",
    "import os, numpy as np, pandas as pd, tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from jiwer import wer\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Enable mixed precision but with float32 accumulation for better stability\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ahs/AHS-uni/Projects/speech-processing/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pnZ_fx7Uf0_d",
    "outputId": "4bde0e2c-d721-4758-8cb1-a5eea470607f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 2362 val size: 263\n",
      "ORIGINAL: train=2362, val=263\n",
      "REDUCED: train=472, val=53\n"
     ]
    }
   ],
   "source": [
    "# Path setup\n",
    "DATA_DIR      = \"/content/LJSpeech/LJSpeech-1.1\"\n",
    "WAVS_PATH     = os.path.join(DATA_DIR, \"wavs\") + \"/\"\n",
    "METADATA_PATH = os.path.join(DATA_DIR, \"metadata.csv\")\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv(METADATA_PATH, sep=\"|\", header=None, quoting=3)\n",
    "df.columns = [\"file_name\",\"transcription\",\"normalized_transcription\"]\n",
    "df = df[[\"file_name\",\"normalized_transcription\"]]\n",
    "\n",
    "# Filter out very long or very short samples for better batch efficiency\n",
    "df['audio_length'] = df['file_name'].apply(lambda x: os.path.getsize(WAVS_PATH + x + '.wav'))\n",
    "df = df[(df['audio_length'] > 10000) & (df['audio_length'] < 200000)]\n",
    "df = df.drop('audio_length', axis=1).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split to train/validation sets\n",
    "split = int(len(df)*0.9)\n",
    "df_train, df_val = df[:split], df[split:]\n",
    "print(\"train size:\", len(df_train), \"val size:\", len(df_val))\n",
    "\n",
    "# Use 20% of the data - increased from 15% for better representation\n",
    "sample_fraction = 0.2\n",
    "df_train = df_train.sample(frac=sample_fraction, random_state=42).reset_index(drop=True)\n",
    "df_val = df_val.sample(frac=sample_fraction, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"ORIGINAL: train={split}, val={len(df) - split}\")\n",
    "print(f\"REDUCED: train={len(df_train)}, val={len(df_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9NOmD8of05e",
    "outputId": "65c91388-bbf3-40aa-a6cd-220707987c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 41\n"
     ]
    }
   ],
   "source": [
    "all_text = \" \".join(df_train.normalized_transcription).lower()\n",
    "chars    = sorted(set(all_text))\n",
    "# 0: CTC blank, 1: PAD, 2: SOS, 3: EOS\n",
    "char_to_idx = {c:i+4 for i,c in enumerate(chars)}\n",
    "char_to_idx.update({\"[blank]\":0, \"[pad]\":1, \"[sos]\":2, \"[eos]\":3})\n",
    "idx_to_char = {i:c for c,i in char_to_idx.items()}\n",
    "VOCAB_SIZE  = len(char_to_idx)\n",
    "print(\"vocab size:\", VOCAB_SIZE)\n",
    "\n",
    "# TF-lookup from string to int\n",
    "keys   = tf.constant(list(char_to_idx.keys()))\n",
    "vals   = tf.constant(list(char_to_idx.values()), tf.int64)\n",
    "char_table = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(keys, vals),\n",
    "    default_value=1  # PAD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ZRHh8oiEf00d"
   },
   "outputs": [],
   "source": [
    "# Optimized spectrogram parameters\n",
    "FRAME_LEN, FRAME_STEP, FFT_LEN = 256, 160, 512  # Increased FFT_LEN for better frequency resolution\n",
    "\n",
    "@tf.function\n",
    "def encode_single_sample(wav_file, label):\n",
    "    # 1) load wav\n",
    "    f = tf.io.read_file(WAVS_PATH + wav_file + \".wav\")\n",
    "    audio, _ = tf.audio.decode_wav(f)\n",
    "    audio = tf.squeeze(audio, -1)\n",
    "    audio = tf.cast(audio, tf.float32)\n",
    "\n",
    "    # 2) More aggressive preprocessing\n",
    "    # Apply a pre-emphasis filter\n",
    "    pre_emphasis = 0.97\n",
    "    audio = tf.concat([audio[:1], audio[1:] - pre_emphasis * audio[:-1]], 0)\n",
    "\n",
    "    # 3) stft with improved parameters\n",
    "    spec = tf.signal.stft(audio, FRAME_LEN, FRAME_STEP, FFT_LEN, window_fn=tf.signal.hann_window)\n",
    "    spec = tf.abs(spec)\n",
    "\n",
    "    # Apply power law compression (cube root instead of square root)\n",
    "    spec = tf.pow(spec, 0.33)\n",
    "\n",
    "    # 4) Better normalization - global normalization with fixed values\n",
    "    # These fixed values help prevent extreme normalization\n",
    "    mean = 0.5  # Fixed mean\n",
    "    std = 0.1   # Fixed std\n",
    "    spec = (spec - mean) / std\n",
    "\n",
    "    # Clip outliers\n",
    "    spec = tf.clip_by_value(spec, -3, 3)\n",
    "\n",
    "    # 5) label → char ids with lower case conversion\n",
    "    txt = tf.strings.lower(label)\n",
    "    chars = tf.strings.unicode_split(txt, \"UTF-8\")\n",
    "    ids = char_table.lookup(chars)\n",
    "    ids = tf.cast(ids, tf.int32)\n",
    "\n",
    "    # 6) prepare CTC labels, decoder in/out\n",
    "    ctc_labels = ids\n",
    "    dec_in = tf.concat([[char_to_idx[\"[sos]\"]], ids], axis=0)\n",
    "    dec_out = tf.concat([ids, [char_to_idx[\"[eos]\"]]], axis=0)\n",
    "\n",
    "    return spec, (ctc_labels, dec_in, dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "vKrmd5bMf5W1"
   },
   "outputs": [],
   "source": [
    "# Increased batch size (from 8 to 16) for better training efficiency\n",
    "BATCH = 16\n",
    "\n",
    "def prepare_dataset(df, is_training=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((\n",
    "        list(df.file_name), list(df.normalized_transcription)\n",
    "    ))\n",
    "\n",
    "    # More efficient mapping with parallel processing\n",
    "    ds = ds.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Cache data if training for faster access\n",
    "    if is_training:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    # Shuffle with a large buffer for better randomization\n",
    "    if is_training:\n",
    "        ds = ds.shuffle(buffer_size=1000, seed=42)\n",
    "\n",
    "    # Padded batch with more efficient padding values\n",
    "    ds = ds.padded_batch(\n",
    "        BATCH,\n",
    "        padded_shapes=(\n",
    "          [None, None],          # spec\n",
    "          ([None], [None], [None])  # ctc, dec_in, dec_out\n",
    "        ),\n",
    "        padding_values=(\n",
    "          0.0,  # spec pad\n",
    "          (\n",
    "            char_to_idx[\"[blank]\"],  # ctc pad\n",
    "            char_to_idx[\"[pad]\"],    # dec_in pad\n",
    "            char_to_idx[\"[pad]\"]     # dec_out pad\n",
    "          )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = prepare_dataset(df_train, is_training=True)\n",
    "val_ds = prepare_dataset(df_val, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "37SRXdpEf9ss"
   },
   "outputs": [],
   "source": [
    "def get_pos_enc(maxlen, dm):\n",
    "    pos = np.arange(maxlen)[:,None]\n",
    "    i = np.arange(dm)[None,:]\n",
    "    angle = pos / np.power(10000, (2*(i//2))/dm)\n",
    "    angle[:,0::2] = np.sin(angle[:,0::2])\n",
    "    angle[:,1::2] = np.cos(angle[:,1::2])\n",
    "    return tf.cast(angle, tf.float32)\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    mask = tf.cast(tf.equal(seq, char_to_idx[\"[pad]\"]), tf.float32)\n",
    "    return mask[:,None,None,:]  # [B,1,1,T]\n",
    "\n",
    "def create_look_ahead_mask(sz):\n",
    "    return 1 - tf.linalg.band_part(tf.ones((sz,sz)), -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "42t5_Njnf9mE"
   },
   "outputs": [],
   "source": [
    "def encoder_layer(dm, nh, pf):\n",
    "    inp = layers.Input((None, dm))\n",
    "    att = layers.MultiHeadAttention(nh, key_dim=dm)(inp, inp)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inp + att)  # Added epsilon\n",
    "    f = layers.Dense(pf, activation=\"relu\")(x)\n",
    "    f = layers.Dense(dm)(f)\n",
    "    out = layers.LayerNormalization(epsilon=1e-6)(x + f)\n",
    "    return keras.Model(inp, out, name=\"enc_layer\")\n",
    "\n",
    "def decoder_layer(dm, nh, pf):\n",
    "    di = layers.Input((None, dm))\n",
    "    eo = layers.Input((None, dm))\n",
    "    la = layers.Input((1,None,None))\n",
    "    pm = layers.Input((1,None,None))\n",
    "    att1 = layers.MultiHeadAttention(nh, key_dim=dm)(\n",
    "              di, di, attention_mask=la)\n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(di + att1)\n",
    "    att2 = layers.MultiHeadAttention(nh, key_dim=dm)(\n",
    "              x1, eo, attention_mask=pm)\n",
    "    x2 = layers.LayerNormalization(epsilon=1e-6)(x1 + att2)\n",
    "    f = layers.Dense(pf, activation=\"relu\")(x2)\n",
    "    f = layers.Dense(dm)(f)\n",
    "    out = layers.LayerNormalization(epsilon=1e-6)(x2 + f)\n",
    "    return keras.Model([di,eo,la,pm], out, name=\"dec_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "xyebDgr8f9jl"
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters - simplified further\n",
    "DM = 96           # Reduced model dimension\n",
    "N_ENC = 2         # Fewer encoder layers\n",
    "N_DEC = 1         # Only one decoder layer\n",
    "NH = 4            # Number of attention heads\n",
    "PF = 256          # Smaller feed-forward dimension\n",
    "FREQ_BINS = FFT_LEN // 2 + 1\n",
    "DROPOUT_RATE = 0.2  # Increased dropout for regularization\n",
    "\n",
    "# 1) Precompute positional encoding\n",
    "pos_enc = get_pos_enc(5000, DM)\n",
    "pos_enc = tf.constant(pos_enc, tf.float32)\n",
    "\n",
    "# 2) Encoder input + projection\n",
    "enc_in = layers.Input(shape=(None, FREQ_BINS), name=\"enc_in\")\n",
    "x = layers.Dense(DM, kernel_initializer='he_normal', name=\"proj_enc\")(enc_in)\n",
    "\n",
    "# 3) Add positional encoding\n",
    "def add_positional_encoding(x):\n",
    "    return x + tf.cast(pos_enc[:tf.shape(x)[1], :], x.dtype)\n",
    "\n",
    "x = layers.Lambda(add_positional_encoding, name=\"add_pos_enc\")(x)\n",
    "x = layers.Dropout(DROPOUT_RATE, name=\"drop_proj\")(x)\n",
    "\n",
    "# 4) Encoder blocks - FIXED WITH UNIQUE NAMES\n",
    "for i in range(N_ENC):\n",
    "    # Create encoder layers with unique names for each component\n",
    "    # Self-attention\n",
    "    att = layers.MultiHeadAttention(\n",
    "              num_heads=NH, key_dim=DM,\n",
    "              name=f\"enc_mha_{i}\"\n",
    "          )(x, x)\n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6, name=f\"enc_ln1_{i}\")(x + att)\n",
    "\n",
    "    # Feed-forward\n",
    "    f = layers.Dense(PF, activation=\"relu\", name=f\"enc_ffn1_{i}\")(x1)\n",
    "    f = layers.Dense(DM, name=f\"enc_ffn2_{i}\")(f)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=f\"enc_ln2_{i}\")(x1 + f)\n",
    "\n",
    "enc_out = x\n",
    "\n",
    "# 5) CTC head\n",
    "ctc_logits = layers.Dense(VOCAB_SIZE, name=\"ctc_logits\")(enc_out)\n",
    "\n",
    "# 6) Decoder input + embed\n",
    "dec_in = layers.Input(shape=(None,), dtype=tf.int32, name=\"dec_in\")\n",
    "y = layers.Embedding(VOCAB_SIZE, DM, name=\"emb_dec\")(dec_in)\n",
    "y = layers.Lambda(add_positional_encoding, name=\"add_pos_dec\")(y)\n",
    "y = layers.Dropout(DROPOUT_RATE, name=\"drop_emb\")(y)\n",
    "\n",
    "# 7) Build masks\n",
    "look_ahead_mask = layers.Lambda(\n",
    "    lambda x: create_look_ahead_mask(tf.shape(x)[1]),\n",
    "    name=\"look_mask\"\n",
    ")(dec_in)\n",
    "\n",
    "pad_mask = layers.Lambda(\n",
    "    lambda x: create_padding_mask(x),\n",
    "    name=\"pad_mask\"\n",
    ")(dec_in)\n",
    "\n",
    "combined_mask = layers.Lambda(\n",
    "    lambda x: tf.maximum(x[0], x[1]),\n",
    "    name=\"combined_mask\"\n",
    ")([look_ahead_mask, pad_mask])\n",
    "\n",
    "# 8) Decoder blocks - ENSURE UNIQUE NAMES\n",
    "for i in range(N_DEC):\n",
    "    # Masked self-attention\n",
    "    att1 = layers.MultiHeadAttention(\n",
    "               num_heads=NH, key_dim=DM,\n",
    "               name=f\"dec_mha1_{i}\"\n",
    "           )(y, y, attention_mask=combined_mask)\n",
    "    y1 = layers.LayerNormalization(epsilon=1e-6, name=f\"dec_ln1_{i}\")(y + att1)\n",
    "\n",
    "    # Cross-attention\n",
    "    att2 = layers.MultiHeadAttention(\n",
    "               num_heads=NH, key_dim=DM,\n",
    "               name=f\"dec_mha2_{i}\"\n",
    "           )(y1, enc_out)\n",
    "    y2 = layers.LayerNormalization(epsilon=1e-6, name=f\"dec_ln2_{i}\")(y1 + att2)\n",
    "\n",
    "    # Feed-forward\n",
    "    ffn = layers.Dense(PF, activation=\"relu\", name=f\"dec_ffn1_{i}\")(y2)\n",
    "    ffn = layers.Dense(DM, name=f\"dec_ffn2_{i}\")(ffn)\n",
    "    y = layers.LayerNormalization(epsilon=1e-6, name=f\"dec_ln3_{i}\")(y2 + ffn)\n",
    "\n",
    "# 9) Output projection\n",
    "dec_logits = layers.Dense(VOCAB_SIZE, name=\"dec_logits\")(y)\n",
    "\n",
    "# 10) Model definition\n",
    "model = keras.Model(inputs=[enc_in, dec_in],\n",
    "                   outputs=[ctc_logits, dec_logits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "cKBX99yTgDdt"
   },
   "outputs": [],
   "source": [
    "# CTC loss function with explicit handling of blank index\n",
    "def ctc_loss_fn(y_true, y_pred):\n",
    "    # Scale predictions more aggressively\n",
    "    y_pred = tf.nn.log_softmax(y_pred, axis=-1)\n",
    "\n",
    "    batch_size = tf.shape(y_pred)[0]\n",
    "    input_length = tf.shape(y_pred)[1]\n",
    "\n",
    "    # Calculate label lengths but ensure they're appropriate\n",
    "    label_length = tf.math.count_nonzero(\n",
    "        tf.not_equal(y_true, 0), # Using 0 for blank directly\n",
    "        axis=1,\n",
    "        dtype=tf.int32\n",
    "    )\n",
    "\n",
    "    # Ensure no zero-length labels\n",
    "    label_length = tf.maximum(label_length, 1)\n",
    "\n",
    "    # Simplify: make input length much shorter - this helps with alignment\n",
    "    input_length = tf.fill([batch_size], tf.minimum(20, input_length))\n",
    "\n",
    "    # Ensure tensors are the right type\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "    # Use sparse CTC loss\n",
    "    loss = tf.nn.ctc_loss(\n",
    "        labels=y_true,\n",
    "        logits=y_pred,\n",
    "        label_length=label_length,\n",
    "        logit_length=input_length,\n",
    "        blank_index=0,  # Using 0 directly\n",
    "        logits_time_major=False\n",
    "    )\n",
    "\n",
    "    # Higher clip value to see actual progress\n",
    "    loss = tf.clip_by_value(loss, 0, 300)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "# Improved sequence loss with masking\n",
    "def seq2seq_loss(y_true, y_pred):\n",
    "    # Create mask to ignore padding tokens\n",
    "    mask = tf.cast(tf.not_equal(y_true, char_to_idx[\"[pad]\"]), tf.float32)\n",
    "\n",
    "    # Also mask [eos] token (optional)\n",
    "    eos_mask = tf.cast(tf.not_equal(y_true, char_to_idx[\"[eos]\"]), tf.float32)\n",
    "    mask = mask * eos_mask\n",
    "\n",
    "    # Cross-entropy loss\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true, y_pred, from_logits=True\n",
    "    )\n",
    "\n",
    "    # Apply mask and calculate mean\n",
    "    return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "R3Pd4OVJgDac",
    "outputId": "32479ff5-02c9-47cb-bd19-d1ee81fcb223"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ enc_in (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ proj_enc (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,768</span> │ enc_in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_pos_enc         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ proj_enc[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ drop_proj (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_pos_enc[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_mha_0           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,704</span> │ drop_proj[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ drop_proj[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ drop_proj[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ enc_mha_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_ln1_0           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │ add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_ffn1_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,832</span> │ enc_ln1_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_ffn2_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,672</span> │ enc_ffn1_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ enc_ln1_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ enc_ffn2_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_ln2_0           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │ add_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_mha_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,704</span> │ enc_ln2_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ enc_ln2_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_in (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ enc_ln2_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ enc_mha_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ emb_dec (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,936</span> │ dec_in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_ln1_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │ add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_pos_dec         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ emb_dec[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ look_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dec_in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pad_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dec_in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│                     │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_ffn1_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,832</span> │ enc_ln1_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ drop_emb (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_pos_dec[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ combined_mask       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ look_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             │            │ pad_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_ffn2_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,672</span> │ enc_ffn1_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_mha1_0          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,704</span> │ drop_emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ drop_emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │                   │            │ combined_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ enc_ln1_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ enc_ffn2_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ drop_emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │                   │            │ dec_mha1_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_ln2_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │ add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_ln1_0           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │ add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_mha2_0          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">148,704</span> │ dec_ln1_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ enc_ln2_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dec_ln1_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ dec_mha2_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_ln2_0           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │ add_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_ffn1_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,832</span> │ dec_ln2_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_ffn2_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,672</span> │ dec_ffn1_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dec_ln2_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ dec_ffn2_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_ln3_0           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │ add_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ ctc_logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,977</span> │ enc_ln2_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,977</span> │ dec_ln3_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ enc_in (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m257\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ proj_enc (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │     \u001b[38;5;34m24,768\u001b[0m │ enc_in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_pos_enc         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ proj_enc[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ drop_proj (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_pos_enc[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_mha_0           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │    \u001b[38;5;34m148,704\u001b[0m │ drop_proj[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ drop_proj[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_14 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ drop_proj[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ enc_mha_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_ln1_0           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │        \u001b[38;5;34m192\u001b[0m │ add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_ffn1_0 (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m24,832\u001b[0m │ enc_ln1_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_ffn2_0 (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │     \u001b[38;5;34m24,672\u001b[0m │ enc_ffn1_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_15 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ enc_ln1_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ enc_ffn2_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_ln2_0           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │        \u001b[38;5;34m192\u001b[0m │ add_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_mha_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │    \u001b[38;5;34m148,704\u001b[0m │ enc_ln2_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ enc_ln2_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_in (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_16 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ enc_ln2_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ enc_mha_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ emb_dec (\u001b[38;5;33mEmbedding\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │      \u001b[38;5;34m3,936\u001b[0m │ dec_in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_ln1_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │        \u001b[38;5;34m192\u001b[0m │ add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_pos_dec         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ emb_dec[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ look_mask (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ dec_in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pad_mask (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ dec_in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│                     │ \u001b[38;5;45mNone\u001b[0m)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_ffn1_1 (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m24,832\u001b[0m │ enc_ln1_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ drop_emb (\u001b[38;5;33mDropout\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_pos_dec[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ combined_mask       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ look_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mLambda\u001b[0m)            │ \u001b[38;5;45mNone\u001b[0m)             │            │ pad_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_ffn2_1 (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │     \u001b[38;5;34m24,672\u001b[0m │ enc_ffn1_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_mha1_0          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │    \u001b[38;5;34m148,704\u001b[0m │ drop_emb[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ drop_emb[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                     │                   │            │ combined_mask[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_17 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ enc_ln1_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ enc_ffn2_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_18 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ drop_emb[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                     │                   │            │ dec_mha1_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_ln2_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │        \u001b[38;5;34m192\u001b[0m │ add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_ln1_0           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │        \u001b[38;5;34m192\u001b[0m │ add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_mha2_0          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │    \u001b[38;5;34m148,704\u001b[0m │ dec_ln1_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ enc_ln2_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_19 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dec_ln1_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ dec_mha2_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_ln2_0           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │        \u001b[38;5;34m192\u001b[0m │ add_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_ffn1_0 (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m24,832\u001b[0m │ dec_ln2_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_ffn2_0 (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │     \u001b[38;5;34m24,672\u001b[0m │ dec_ffn1_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_20 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dec_ln2_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ dec_ffn2_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_ln3_0           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)  │        \u001b[38;5;34m192\u001b[0m │ add_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ ctc_logits (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m)  │      \u001b[38;5;34m3,977\u001b[0m │ enc_ln2_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_logits (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m)  │      \u001b[38;5;34m3,977\u001b[0m │ dec_ln3_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">781,330</span> (2.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m781,330\u001b[0m (2.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">781,330</span> (2.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m781,330\u001b[0m (2.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: DM=96, N_ENC=2, N_DEC=1, NH=4, PF=256\n",
      "Learning rate: 5e-05\n",
      "Optimizer: Adam with gradient clipping (clipnorm=1.0)\n",
      "Loss weights: CTC=0.4, Decoder=0.6\n",
      "Batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# Define a custom learning rate scheduler for better convergence\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"d_model\": self.d_model,\n",
    "            \"warmup_steps\": self.warmup_steps\n",
    "        }\n",
    "\n",
    "# Fixed and improved CTC loss function\n",
    "def ctc_loss_fn(y_true, y_pred):\n",
    "    # Apply log softmax for numerical stability\n",
    "    y_pred = tf.nn.log_softmax(y_pred, axis=-1)\n",
    "\n",
    "    batch_size = tf.shape(y_pred)[0]\n",
    "    input_length = tf.shape(y_pred)[1]\n",
    "\n",
    "    # Ensure label lengths are appropriate\n",
    "    label_length = tf.math.count_nonzero(\n",
    "        tf.not_equal(y_true, char_to_idx[\"[blank]\"]),\n",
    "        axis=1,\n",
    "        dtype=tf.int32\n",
    "    )\n",
    "    label_length = tf.maximum(label_length, 1)  # Avoid zero lengths\n",
    "\n",
    "    # Ensure input lengths aren't too long compared to labels\n",
    "    # This helps prevent CTC alignment issues\n",
    "    max_label_len = tf.reduce_max(label_length)\n",
    "    input_length = tf.fill([batch_size], tf.minimum(input_length, max_label_len * 4))\n",
    "\n",
    "    # Ensure tensors are the right type\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "    # Use CTC loss with better parameters\n",
    "    loss = tf.nn.ctc_loss(\n",
    "        labels=y_true,\n",
    "        logits=y_pred,\n",
    "        label_length=label_length,\n",
    "        logit_length=input_length,\n",
    "        blank_index=char_to_idx[\"[blank]\"],\n",
    "        logits_time_major=False\n",
    "    )\n",
    "\n",
    "    # Clip to avoid extreme values\n",
    "    loss = tf.clip_by_value(loss, 0, 100)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Use a smaller learning rate\n",
    "learning_rate = 5e-5  # Reduced from 1e-4\n",
    "\n",
    "# Create optimizer with gradient clipping\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.98,\n",
    "    epsilon=1e-9,\n",
    "    clipnorm=0.5  # More aggressive gradient clipping\n",
    ")\n",
    "\n",
    "# Initially focus only on CTC loss (disable decoder)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={\n",
    "        \"ctc_logits\": ctc_loss_fn,\n",
    "        \"dec_logits\": seq2seq_loss\n",
    "    },\n",
    "    loss_weights={\n",
    "        \"ctc_logits\": 1.0,  # Only use CTC loss initially\n",
    "        \"dec_logits\": 0.0   # Disable decoder loss\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Print training configuration for reference\n",
    "print(f\"Model parameters: DM={DM}, N_ENC={N_ENC}, N_DEC={N_DEC}, NH={NH}, PF={PF}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Optimizer: Adam with gradient clipping (clipnorm=1.0)\")\n",
    "print(f\"Loss weights: CTC={0.4}, Decoder={0.6}\")\n",
    "print(f\"Batch size: {BATCH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEXgXE9sgvv_",
    "outputId": "8a25b4a8-f738-4d8d-a93f-2ce174951e51"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Using 472 training samples and 53 validation samples\n",
      "Model dimensions: DM=96, Encoder layers=2, Decoder layers=1\n",
      "Epoch 1/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - ctc_logits_loss: 100.0000 - dec_logits_loss: 4.4008 - loss: 100.0000 \n",
      "Epoch 1: val_loss improved from inf to 100.00000, saving model to speech_model_checkpoint.keras\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 13s/step - ctc_logits_loss: 100.0000 - dec_logits_loss: 4.4010 - loss: 100.0000 - val_ctc_logits_loss: 100.0000 - val_dec_logits_loss: 4.3987 - val_loss: 100.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - ctc_logits_loss: 100.0000 - dec_logits_loss: 4.4144 - loss: 100.0000 \n",
      "Epoch 2: val_loss did not improve from 100.00000\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 11s/step - ctc_logits_loss: 100.0000 - dec_logits_loss: 4.4143 - loss: 100.0000 - val_ctc_logits_loss: 100.0000 - val_dec_logits_loss: 4.3987 - val_loss: 100.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m 4/30\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:18\u001b[0m 5s/step - ctc_logits_loss: 100.0000 - dec_logits_loss: 4.4143 - loss: 100.0000"
     ]
    }
   ],
   "source": [
    "# Data preparation helper function\n",
    "def pack_for_fit(spectrogram, labels):\n",
    "    ctc_labels, dec_inp, dec_out = labels\n",
    "    x = (spectrogram, dec_inp)\n",
    "    y = (ctc_labels, dec_out)\n",
    "    return x, y\n",
    "\n",
    "# Prepare datasets\n",
    "train_ds_fit = train_ds.map(pack_for_fit, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds_fit = val_ds.map(pack_for_fit, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Enhanced callbacks for better training\n",
    "# Fix: Add .keras extension to checkpoint filepath\n",
    "checkpoint_path = \"speech_model_checkpoint.keras\"\n",
    "\n",
    "# Simple callbacks to avoid conflicts\n",
    "callbacks = [\n",
    "    # Model checkpoint to save best model\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Early stopping with patience\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Add a callback to reduce batch size if needed\n",
    "    tf.keras.callbacks.TerminateOnNaN()\n",
    "]\n",
    "\n",
    "# Clear backend session and free memory before training\n",
    "import gc\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Disable XLA JIT compilation to avoid CTC loss errors\n",
    "tf.config.optimizer.set_jit(False)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Using {len(df_train)} training samples and {len(df_val)} validation samples\")\n",
    "print(f\"Model dimensions: DM={DM}, Encoder layers={N_ENC}, Decoder layers={N_DEC}\")\n",
    "\n",
    "# Train with fewer epochs initially\n",
    "history = model.fit(\n",
    "    train_ds_fit,\n",
    "    validation_data=val_ds_fit,\n",
    "    epochs=10,  # Reduced number of epochs\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "model.save(\"speech_recognition_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rr0gQLligxhq"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot individual losses if available\n",
    "plt.subplot(1, 2, 2)\n",
    "if 'ctc_logits_loss' in history.history:\n",
    "    plt.plot(history.history['ctc_logits_loss'], label='CTC Loss')\n",
    "    plt.plot(history.history['dec_logits_loss'], label='Decoder Loss')\n",
    "    plt.title('Component Losses')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Component losses not available',\n",
    "             horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dzm0XxqGgzFm"
   },
   "outputs": [],
   "source": [
    "def greedy_decode(spect):\n",
    "    \"\"\"Optimized greedy decoding function\"\"\"\n",
    "    # Add batch dimension\n",
    "    s = tf.expand_dims(spect, 0)  # [1,T,F]\n",
    "\n",
    "    # Get encoder output - run through encoder\n",
    "    enc_output = None\n",
    "    x = model.get_layer(\"proj_enc\")(s)\n",
    "    x = model.get_layer(\"add_pos_enc\")(x)\n",
    "    x = model.get_layer(\"drop_proj\")(x)\n",
    "\n",
    "    # Run through encoder layers\n",
    "    for i in range(N_ENC):\n",
    "        for layer in model.layers:\n",
    "            if layer.name == f\"enc_layer_{i}\":\n",
    "                x = layer(x)\n",
    "                break\n",
    "\n",
    "    enc_output = x\n",
    "\n",
    "    # CTC greedy decoding\n",
    "    ctc_logits = model.get_layer(\"ctc_logits\")(enc_output)\n",
    "    ctc_logits = tf.cast(ctc_logits, tf.float32)  # Ensure float32\n",
    "\n",
    "    # Transpose for ctc_greedy_decoder\n",
    "    ctc_t = tf.transpose(ctc_logits, [1, 0, 2])\n",
    "\n",
    "    # Get sequence length\n",
    "    seq_len = tf.fill([1], tf.shape(ctc_logits)[1])\n",
    "\n",
    "    # Run greedy decoder\n",
    "    decoded, _ = tf.nn.ctc_greedy_decoder(ctc_t, seq_len)\n",
    "\n",
    "    # Convert to dense\n",
    "    seq = tf.sparse.to_dense(decoded[0])[0].numpy().tolist()\n",
    "\n",
    "    # Convert to text - filtering out special tokens\n",
    "    ctc_str = \"\".join(idx_to_char[i] for i in seq if i > 3)\n",
    "\n",
    "    # Autoregressive decoding\n",
    "    di = [char_to_idx[\"[sos]\"]]\n",
    "    max_len = 200  # Maximum sequence length\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        # Get output for current sequence\n",
    "        dec_input = tf.expand_dims(di, 0)  # Add batch dimension\n",
    "\n",
    "        # Get decoder logits\n",
    "        out = model.predict([s, dec_input], verbose=0)[1]\n",
    "\n",
    "        # Get most likely next token\n",
    "        next_token = tf.argmax(out[0, -1, :], axis=-1).numpy()\n",
    "\n",
    "        # Break if EOS token\n",
    "        if next_token == char_to_idx[\"[eos]\"]:\n",
    "            break\n",
    "\n",
    "        # Add next token to sequence\n",
    "        di.append(next_token)\n",
    "\n",
    "    # Convert sequence to text, filtering out special tokens\n",
    "    seq_str = \"\".join(idx_to_char[i] for i in di[1:]\n",
    "                     if idx_to_char[i] not in [\"[pad]\", \"[eos]\", \"[sos]\", \"[blank]\"])\n",
    "\n",
    "    return ctc_str, seq_str\n",
    "\n",
    "# Function to calculate Word Error Rate\n",
    "def calculate_wer(reference, hypothesis):\n",
    "    return wer(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNP6PX7Yg2Dd"
   },
   "outputs": [],
   "source": [
    "# Test on multiple validation samples\n",
    "num_samples = 5\n",
    "total_wer_ctc = 0\n",
    "total_wer_seq = 0\n",
    "\n",
    "print(\"Evaluating model on validation samples...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for spec, (_, _, dec_out) in val_ds.take(num_samples):\n",
    "    spec = spec[0].numpy()\n",
    "    true_ids = dec_out[0].numpy()\n",
    "    true_str = \"\".join(idx_to_char[i] for i in true_ids if i > 3 and i != char_to_idx[\"[eos]\"])\n",
    "\n",
    "    p_ctc, p_seq = greedy_decode(spec)\n",
    "\n",
    "    # Calculate WER\n",
    "    wer_ctc = calculate_wer(true_str, p_ctc)\n",
    "    wer_seq = calculate_wer(true_str, p_seq)\n",
    "\n",
    "    total_wer_ctc += wer_ctc\n",
    "    total_wer_seq += wer_seq\n",
    "\n",
    "    print(\"True     :\", true_str)\n",
    "    print(\"CTC pred :\", p_ctc)\n",
    "    print(\"Seq pred :\", p_seq)\n",
    "    print(f\"WER(CTC) : {wer_ctc:.4f}\")\n",
    "    print(f\"WER(Seq) : {wer_seq:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Calculate average WER\n",
    "avg_wer_ctc = total_wer_ctc / num_samples\n",
    "avg_wer_seq = total_wer_seq / num_samples\n",
    "\n",
    "print(f\"Average WER (CTC): {avg_wer_ctc:.4f}\")\n",
    "print(f\"Average WER (Seq): {avg_wer_seq:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWpdn8_7g4Bl"
   },
   "outputs": [],
   "source": [
    "# Optional: Function to process user-uploaded audio files\n",
    "def process_audio_file(audio_file_path):\n",
    "    \"\"\"Process an uploaded audio file for inference\"\"\"\n",
    "    # Read the audio file\n",
    "    audio, sr = tf.audio.decode_wav(\n",
    "        tf.io.read_file(audio_file_path)\n",
    "    )\n",
    "    audio = tf.squeeze(audio, -1)\n",
    "\n",
    "    # Resample to 22050 Hz if needed\n",
    "    if sr != 22050:\n",
    "        # Implement resampling here if needed\n",
    "        pass\n",
    "\n",
    "    # Convert to spectrogram\n",
    "    spec = tf.signal.stft(audio, FRAME_LEN, FRAME_STEP, FFT_LEN)\n",
    "    spec = tf.abs(spec)\n",
    "    spec = tf.pow(spec, 0.5)\n",
    "\n",
    "    # Normalize\n",
    "    mean = tf.reduce_mean(spec, axis=1, keepdims=True)\n",
    "    std = tf.math.reduce_std(spec, axis=1, keepdims=True) + 1e-6\n",
    "    spec = (spec - mean) / std\n",
    "\n",
    "    # Get transcriptions\n",
    "    ctc_pred, seq_pred = greedy_decode(spec)\n",
    "\n",
    "    return {\n",
    "        \"CTC Prediction\": ctc_pred,\n",
    "        \"Sequence Prediction\": seq_pred\n",
    "    }\n",
    "\n",
    "print(\"Model ready for inference. Upload an audio file or use sample validation data.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
