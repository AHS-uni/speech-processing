dataset_dir: data/LJSpeech
splits_dir: data/splits
preprocessed_dir: data/preprocessed
stats_dir: data/stats
tokenizer_dir: data/preprocessed/tokenizer
checkpoint_dir: checkpoints
artifact_dir: artifacts

random_seed: 61

num_workers: 8

audio:
  n_fft: 1024
  hop_length: 256
  n_mels: 80

tokenization:
  vocab_size: 1000
  character_coverage: 1.0
  model_type: bpe

splits:
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1

model:
  input_size: ${audio.n_mels}
  output_size: ${tokenization.vocab_size}
  d_model: 256
  nhead: 4
  num_encoder_layers: 4
  num_decoder_layers: 3
  dim_feedforward: 512
  dropout: 0.2
  subsample_rate: 4

training:
  batch_size: 32
  learning_rate: 1e-4
  weight_decay: 1e-5
  alpha_start: 0.3
  alpha_end: 0.05
  num_epochs: 20
  scheduler:
    type: cosine
    T_max: ${training.num_epochs}

amp: true

checkpoint:
  save_best_by: wer
  save_interval: 1
